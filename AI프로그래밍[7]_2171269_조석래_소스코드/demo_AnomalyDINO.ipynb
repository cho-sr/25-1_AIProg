{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS 장치가 사용 가능한가? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"MPS 장치가 사용 가능한가? {torch.backends.mps.is_available()}\") \n",
    "device = torch.device(\"mps\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T20:24:31.173424Z",
     "start_time": "2025-06-18T20:24:31.169717Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: AnomalyDINO: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls AnomalyDINO\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T20:29:14.530385Z",
     "start_time": "2025-06-18T20:29:14.389509Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:24:31.179614Z",
     "start_time": "2025-06-18T20:24:31.175512Z"
    }
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import faiss\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from src.detection import augment_image\n",
    "from src.utils import resize_mask_img, get_dataset_info\n",
    "from src.backbones import get_model\n",
    "\n",
    "data_root = \"data\"\n",
    "mvtec_path = f\"{data_root}/mvtec_anomaly_detection\"\n",
    "objects, object_anomalies, masking_default, rotation_default = get_dataset_info(\"MVTec\", \"informed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:24:31.189708Z",
     "start_time": "2025-06-18T20:24:31.182793Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_single_image(image_test, image_ref_list, model, masking = True, rotation = True, use_faiss = True):\n",
    "    features_ref = []\n",
    "\n",
    "    # Extract reference features and set up the knn index (memory bank)\n",
    "    for image_ref in image_ref_list:\n",
    "        if rotation:\n",
    "            img_augmented = augment_image(image_ref)\n",
    "        else:\n",
    "            img_augmented = [image_ref]\n",
    "\n",
    "        for i in range(len(img_augmented)):\n",
    "            image_ref = img_augmented[i]\n",
    "            image_ref_tensor, _ = model.prepare_image(image_ref)\n",
    "            features_ref_i = model.extract_features(image_ref_tensor)\n",
    "            features_ref.append(features_ref_i)\n",
    "\n",
    "    features_ref = np.concatenate(features_ref, axis=0)\n",
    "    if use_faiss:\n",
    "        knn_index1 = faiss.IndexFlatL2(features_ref.shape[1])\n",
    "        faiss.normalize_L2(features_ref)\n",
    "        knn_index1.add(features_ref)\n",
    "    else:\n",
    "        knn_index1 = NearestNeighbors(n_neighbors=1, metric=\"cosine\")\n",
    "        # normalize features\n",
    "        features_ref = features_ref / np.linalg.norm(features_ref, axis=1)[:, None]\n",
    "        knn_index1.fit(features_ref)\n",
    "\n",
    "    # Extract test features\n",
    "    image_tensor_test, grid_size2 = model.prepare_image(image_test)\n",
    "    features_test = model.extract_features(image_tensor_test)\n",
    "\n",
    "    if use_faiss:\n",
    "        faiss.normalize_L2(features_test)\n",
    "        distances, _ = knn_index1.search(features_test, k = 1)\n",
    "        distances = distances / 2\n",
    "    else:\n",
    "        # normalize features\n",
    "        features_test = features_test / np.linalg.norm(features_test, axis=1)[:, None]\n",
    "        distances, _ = knn_index1.kneighbors(features_test, n_neighbors=1)\n",
    "        \n",
    "    # Filter out the relevant patches of test image\n",
    "    if masking:\n",
    "        mask2 = model.compute_background_mask_from_image(image_test, threshold=10, masking_type=masking)\n",
    "        distances[~mask2] = 0.0\n",
    "    else:\n",
    "        mask2 = np.ones(features_test.shape[0], dtype=bool)\n",
    "\n",
    "    # Compute anomaly score\n",
    "    score_top1p = np.mean(sorted(distances, reverse = True)[:int(max(1,len(distances) * 0.01))])\n",
    "    distances = distances.reshape(grid_size2)\n",
    "    return score_top1p, distances, mask2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch-based Anomaly Detection with AnomalyDINO (Demo)\n",
    "\n",
    "## One-shot AD (classical)\n",
    "1. Extract nominal features from reference sample\n",
    "   - select relevant features by masking (optional, dependent on category, texture or object etc.)\n",
    "   - augment by meaningful augmentations (here: rotation), optional\n",
    "2. Extract features from test sample\n",
    "   - select relevant features by masking, optional\n",
    "3. Compute distances between test patches and closest nominal patch\n",
    "   - anomaly score: max(patch_distances), some high quantile, etc.\n",
    "   - anomaly map: upscale & smooth the patch distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:24:32.722459Z",
     "start_time": "2025-06-18T20:24:31.210054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: dinov2_vits14\n",
      "Device: mps\n",
      "Smaller edge size: 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/joseoglae/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv2 model\n",
    "model = get_model('dinov2_vits14', 'mps', smaller_edge_size=32*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:24:32.764049Z",
     "start_time": "2025-06-18T20:24:32.723574Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@7661.821] global loadsave.cpp:268 findDecoder imread_('data/mvtec_anomaly_detection/hazelnut/train/good/001.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31merror\u001B[39m                                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m img_name = \u001B[33m\"\u001B[39m\u001B[33m001.png\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      4\u001B[39m image_ref = cv2.imread(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmvtec_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobject_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/train/good/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimg_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m image_ref = \u001B[43mcv2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcvtColor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_ref\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCOLOR_BGR2RGB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m masking = masking_default[object_name] \u001B[38;5;66;03m# set masking = False for textures\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Compute features and mask\u001B[39;00m\n",
      "\u001B[31merror\u001B[39m: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Load reference image\n",
    "object_name = \"hazelnut\" # change object/image as you like\n",
    "img_name = \"001.png\"\n",
    "image_ref = cv2.imread(f\"{mvtec_path}/{object_name}/train/good/{img_name}\")\n",
    "image_ref = cv2.cvtColor(image_ref, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "masking = masking_default[object_name] # set masking = False for textures\n",
    "\n",
    "# Compute features and mask\n",
    "image_tensor1, grid_size1 = model.prepare_image(image_ref)\n",
    "features_ref = model.extract_features(image_tensor1)\n",
    "mask_ref = model.compute_background_mask(features_ref, grid_size1, threshold=10, masking_type=masking)\n",
    "vis_image_ref = model.get_embedding_visualization(features_ref, grid_size1)\n",
    "\n",
    "# Visualization: 1 = clean reference image, 2 = PCA-dimensions (top 3) of features, 3 = zero-shot masking (PCA[0] > 10)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax1.imshow(image_ref)\n",
    "ax2.imshow(vis_image_ref)\n",
    "resized_mask = resize_mask_img(mask_ref, image_ref.shape, grid_size1)\n",
    "ax3.imshow(image_ref)\n",
    "# prepare_mask\n",
    "ax3.imshow(resize_mask_img(mask_ref, image_ref.shape, grid_size1), alpha=0.3)\n",
    "# ax3.imshow(model.prepare_mask(mask_ref, image_ref.shape, grid_size1), alpha=0.3)\n",
    "fig.tight_layout()\n",
    "ax1.axis('off')\n",
    "ax1.set_title(\"Reference Image\")\n",
    "ax2.axis('off')\n",
    "ax2.set_title(\"PCA of Features\")\n",
    "ax3.axis('off')\n",
    "ax3.set_title(\"Zero-shot Masking\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test image\n",
    "anomaly_type = \"hole\" # \"good\", \"crack\", \"hole\"... (depends on the object/product)\n",
    "test_img_name = \"006.png\"\n",
    "\n",
    "# Load test image from given anomaly type\n",
    "image_test_path = f\"{mvtec_path}/{object_name}/test/{anomaly_type}/{test_img_name}\"\n",
    "image_test = cv2.cvtColor(cv2.imread(image_test_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Compute features and mask\n",
    "image_tensor2, grid_size2 = model.prepare_image(image_test)\n",
    "features_test = model.extract_features(image_tensor2)\n",
    "mask_test = model.compute_background_mask(features_test, grid_size2, threshold=10, masking_type=masking)\n",
    "vis_image_test = model.get_embedding_visualization(features_test, grid_size2)\n",
    "\n",
    "# Visualization: 1 = clean reference image, 2 = PCA-dimensions (top 3) of features, 3 = zero-shot masking (PCA[0] > 10)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax1.imshow(image_test)\n",
    "ax2.imshow(vis_image_test)\n",
    "resized_mask = resize_mask_img(mask_test, image_test.shape, grid_size2)\n",
    "ax3.imshow(image_test)\n",
    "ax3.imshow(resize_mask_img(mask_test, image_test.shape, grid_size2), alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "ax1.axis('off')\n",
    "ax1.set_title(\"Test Image\")\n",
    "ax2.axis('off')\n",
    "ax2.set_title(\"PCA of Features\")\n",
    "ax3.axis('off')\n",
    "ax3.set_title(\"Zero-shot Masking\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in k reference images (k-shot)\n",
    "k = 4\n",
    "image_ref_list = []\n",
    "\n",
    "for i in range(k):\n",
    "    image_ref_path = f\"{mvtec_path}/{object_name}/train/good/{i+1:03d}.png\"\n",
    "    image_ref = cv2.cvtColor(cv2.imread(image_ref_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "    image_ref_list.append(image_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score, patch_distances, mask_test = score_single_image(image_test, image_ref_list, model, masking = masking, rotation = False, use_faiss = True)\n",
    "print(f\"The predict anomaly score ({k}-shot) for this sample is \", anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot anomaly map\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "neon_violet = (0.5, 0.1, 0.5, 0.4)\n",
    "neon_yellow = (0.8, 1.0, 0.02, 0.7)\n",
    "colors = [(1.0, 1, 1.0, 0.0),  neon_violet, neon_yellow]\n",
    "cmap = LinearSegmentedColormap.from_list(\"AnomalyMap\", colors, N=256)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_test, alpha=1.0)\n",
    "\n",
    "d = patch_distances.flatten()\n",
    "d = cv2.resize(d.reshape(grid_size2), (image_test.shape[1], image_test.shape[0]), interpolation = cv2.INTER_LINEAR)\n",
    "d = gaussian_filter(d, sigma=4.0)\n",
    "vmax = 0.5 # vmax may need to be adjusted differently for different objects, for hazelnut 0.5 is a good choice\n",
    "plt.colorbar(plt.imshow(d, alpha=1.0, cmap=cmap, vmax=vmax), orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(f\"Test sample ({object_name}/{anomaly_type}) with anomaly score: {anomaly_score:.3f}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
